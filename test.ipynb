{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, Wav2Vec2BertModel\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(\"facebook/w2v-bert-2.0\")\n",
    "# model = Wav2Vec2BertModel.from_pretrained(\"facebook/w2v-bert-2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 271250])\n",
      "torch.Size([2, 98413])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[-2.7274, -2.6149, -2.5163,  ..., -2.1149, -2.1674, -2.1311],\n",
       "         [-2.0149, -1.9489, -1.7825,  ..., -1.7384, -1.8621, -1.6986],\n",
       "         [-1.6377, -1.3883, -1.1396,  ..., -1.7544, -1.7077, -1.7296],\n",
       "         ...,\n",
       "         [-1.0482, -0.8214, -0.3950,  ..., -1.4695, -1.6222, -1.6071],\n",
       "         [-1.1254, -0.7901, -0.4701,  ..., -1.6016, -1.6721, -1.5878],\n",
       "         [-0.9931, -0.6387, -0.3397,  ...,  1.0000,  1.0000,  1.0000]]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]],\n",
       "       dtype=torch.int32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sampling_rate = torchaudio.load(\"ennuref.wav\")\n",
    "print(waveform.shape)\n",
    "waveform = torchaudio.functional.resample(waveform, orig_freq=sampling_rate, new_freq=16000)\n",
    "print(waveform.shape)\n",
    "inputs = processor(waveform.numpy()[0], sampling_rate=16000, return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 307, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 188, 160])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 307, 160])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from languagecodec_decoder.feature_extractors import EncodecFeatures\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "with open(\"checkpoints/pretrained_language_codec/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# model = EncodecFeatures(**config['model']['init_args'][\"feature_extractor\"][\"init_args\"])\n",
    "\n",
    "# state_dict = torch.load(\"checkpoints/pretrained_language_codec/languagecodec_paper.ckpt\", map_location=\"cpu\")['state_dict']\n",
    "\n",
    "# feature_extractor_state_dict = {}\n",
    "# for k, v in state_dict.items():\n",
    "#     if k.startswith('feature_extractor.'):\n",
    "#         feature_extractor_state_dict[k.replace('feature_extractor.', '')] = v\n",
    "\n",
    "# model.load_state_dict(feature_extractor_state_dict)\n",
    "\n",
    "# del state_dict, feature_extractor_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model']['init_args'][\"backbone\"][\"init_args\"][\"input_channels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhnguyenhoang/Desktop/vscode/speech-language-foundation/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhnguyenhoang/Desktop/vscode/speech-language-foundation/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/Users/anhnguyenhoang/Desktop/vscode/speech-language-foundation/languagecodec_decoder/pretrained.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_raw = torch.load(model_path, map_location=\"cpu\")['state_dict']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from languagecodec_encoder.utils import convert_audio\n",
    "import torchaudio\n",
    "import torch\n",
    "from languagecodec_decoder.pretrained import Vocos\n",
    "\n",
    "device=torch.device('cpu')\n",
    "\n",
    "config_path = \"checkpoints/pretrained_language_codec/config.yaml\"\n",
    "model_path = \"checkpoints/pretrained_language_codec/model.ckpt\"\n",
    "languagecodec = Vocos.from_pretrained0802(config_path, model_path)\n",
    "languagecodec = languagecodec.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 90837])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 284])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav, sr = torchaudio.load(\"ref_en_nu.wav\")\n",
    "wav = convert_audio(wav, sr, 24000, 1) \n",
    "bandwidth_id = torch.tensor([0])\n",
    "wav=wav.to(device)\n",
    "_,discrete_code= languagecodec.encode_infer(torch.cat([wav, wav]), bandwidth_id=bandwidth_id)\n",
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languagecodec.feature_extractor.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 462])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 462])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,discrete_code= languagecodec.encode_infer(torch.cat([wav, wav]), bandwidth_id=bandwidth_id)\n",
    "print(discrete_code.shape)\n",
    "_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 462])\n"
     ]
    }
   ],
   "source": [
    "print(discrete_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 462])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 462])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = languagecodec.codes_to_features(discrete_code)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2876, -0.0315,  0.1397,  0.2557, -0.0260,  0.1442, -0.0943,  0.0470,\n",
       "        -0.4271,  0.3227])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0,0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2876, -0.0315,  0.1397,  0.2557, -0.0260,  0.1442, -0.0943,  0.0470,\n",
       "        -0.4271,  0.3227])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0,0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0139, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = torch.nn.Embedding(5, 16)\n",
    "\n",
    "embed(torch.tensor([[1,2],[3,4]])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [3, 4]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "x = torch.tensor([[[1,2],[3,4]], [[1,2],[3,4]], [[1,2],[3,4]]])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([0.5, 0.5])\n",
    "\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncodecModel, AutoProcessor\n",
    "from models.codec_encoder import EncodecEncoderQuantizer\n",
    "\n",
    "# model = EncodecEncoderQuantizer.from_pretrained(\"facebook/encodec_24khz\")\n",
    "\n",
    "# model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncodecFeatureExtractor {\n",
       "  \"chunk_length_s\": null,\n",
       "  \"feature_extractor_type\": \"EncodecFeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"overlap\": null,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 24000\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100\n",
      "torch.Size([2, 271250])\n",
      "torch.Size([2, 147620])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sampling_rate = torchaudio.load(\"ennuref.wav\")\n",
    "print(sampling_rate)\n",
    "print(waveform.shape)\n",
    "waveform = torchaudio.functional.resample(waveform, orig_freq=sampling_rate, new_freq=processor.sampling_rate)\n",
    "print(waveform.shape)\n",
    "inputs = processor([waveform.numpy()[0], waveform.numpy()[0][:-5000], waveform.numpy()[0][:-500]], sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n",
    "# audio_embed, codes_mask, codes = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32),\n",
       " 'input_values': tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4510e-04,\n",
       "           1.4499e-04, 5.9232e-06]],\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {}\n",
    "x.update(inputs)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 462, 128])\n",
      "torch.Size([3, 462, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = audio_embed*codes_mask\n",
    "\n",
    "print(codes_mask.shape)\n",
    "print(hidden_states.shape)\n",
    "\n",
    "pooled_output = hidden_states.sum(dim=1) / codes_mask.sum(dim=1) # B, I\n",
    "\n",
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462., 462.,\n",
       "         462., 462., 462., 462., 462., 462., 462., 462.],\n",
       "        [449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449., 449.,\n",
       "         449., 449., 449., 449., 449., 449., 449., 449.],\n",
       "        [461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461., 461.,\n",
       "         461., 461., 461., 461., 461., 461., 461., 461.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_mask.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "PredictionOutput(predictions=np.array([1,2,3]), label_ids=np.array([1,2,2]), metrics={\"a\": 1.0}).metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 462, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7727,  8.4915, -3.5145,  ..., -0.6387, -1.6899,  1.7921],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         ...,\n",
       "         [-0.5899,  8.6741, -3.5319,  ..., -0.5725, -1.6565,  1.6399],\n",
       "         [-0.5192,  8.7648, -3.5058,  ..., -0.5464, -1.6215,  1.6735],\n",
       "         [-0.7237,  8.6417, -3.5161,  ..., -0.6146, -1.5997,  1.6597]],\n",
       "\n",
       "        [[-0.7727,  8.4915, -3.5145,  ..., -0.6387, -1.6899,  1.7921],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7727,  8.4915, -3.5145,  ..., -0.6387, -1.6899,  1.7921],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         ...,\n",
       "         [-0.5899,  8.6741, -3.5319,  ..., -0.5725, -1.6565,  1.6399],\n",
       "         [-0.7237,  8.6417, -3.5161,  ..., -0.6146, -1.5997,  1.6597],\n",
       "         [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embed*codes_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 462])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  62,   62,   62,   62,  408,  408,  408,  724,  578,  669,  620,  562,\n",
       "          555,  648,  621,  778,  738,  339,  709,  985,  346,  705,  214,   86,\n",
       "          975,  778,  238,  953,  574,  744,  160,  160,  574,  753,  861,  865,\n",
       "          738,  705,    6,  705,  435,  560,  169,  560,  143,  237,  145,  145,\n",
       "          145,  145,  254,   69,  378,  914,   74,   43, 1023,   43,  698,  857,\n",
       "          328,  561,  344, 1010,   52,  106,  121,  408,  408,  408,  408,  408,\n",
       "          408,  408,  408,  408,  408,  408,   62,   62,   62,   62,   62,   62,\n",
       "           62,   62,   62,   62,   62,   62,   62,   62,   62,   62,   62,   62,\n",
       "           62,   62,   62,   62,   62,   62,   62,   62,   62,   62,   62,  106,\n",
       "          499,  499,  499,  499,  776,  147,  469,  250,  842,  757,  749,  406,\n",
       "           58,  757,  250,  757,  877,  918,  913,  746,  219,  918,   24,  913,\n",
       "          746,  611,  801,  908,  455,  820,  553,  553,  743,  743,  837,  203,\n",
       "          280,  695,  747,  240,  128,  476,  768,  596,  312,  476,  596,  312,\n",
       "          578,  124,  124,  328,  328,  751,  255,  182,  200,  253, 1012,  801,\n",
       "          908,  801,  908,  801,   24,  698,  934,  744,   47,   47,   47,   47,\n",
       "          160,   47,  373,  865,  865,  738,  408,  562,   10,  721,  560,  725,\n",
       "          531,  605,  868,  151,  145,  145,  145,  145,  145,  103,  328,  749,\n",
       "          280,  596,  322,  476,  755,  467,  280,  725,  467,  280,  197,  312,\n",
       "          124,  255,  255,  432,  106,   62,  106,  106,  408,  408,  408,  408,\n",
       "          408,  408,  408,  408,  408,  738,  408,  488,  619,  840,  917,  425,\n",
       "           61,  882,  882,  744,   47,   47,   47,  709,  865,  865,  876,  171,\n",
       "          602,  431,   10,  352,  434,  716,  771,  429,  253,  743,   43,  967,\n",
       "          432,  115,  250,  980,   74,  124,  744,   47,   47,   47,   47,   47,\n",
       "           47,   47,  934,   69, 1006,  562,  725,  893,  487,  749,  749,  749,\n",
       "          749,  749,  771,  771,  392,  235,  235,    3,  311,  868,  476,  235,\n",
       "          539,  978,  573,  865,  922,  160,  744,  744,   73,  922,  865,   85,\n",
       "          328,  952,  487,  487,  578,  749,  749, 1006, 1006,  768,  655,  596,\n",
       "           58,  578,  147,  768,  562,  725,   10,  462,  578,  406,  725,  842,\n",
       "         1006,  462,  725,  655, 1006, 1006,  560,  725,  580,  573,  788,  586,\n",
       "          875,  311,  751,  143,   69,  246,  321,  876,  410,  865,  537,  677,\n",
       "          499,  537,  537,  537,  537,  779,  779,  779,  475,  475,  106,  738,\n",
       "         1017,  408,  876,  131,  876,   44,  184,  542,  707,  529,  297,   19,\n",
       "          757,  200,  977,  879,  224,  370, 1004,  776,  328,  959,   68,  959,\n",
       "          200,  759,  200,  759,  406,  311,  583,  432,  276,  857,  728,   84,\n",
       "          852,  425,   41,  842,  757,  147,  124,  124,  776,  768,  655, 1006,\n",
       "          749,  462,  842,  655,  188,  250,  655,  728,  406,   58,  857,  757,\n",
       "           69,  768,  689,  312,  580,  167,  379,  533,  293,  561,  751,  344,\n",
       "          561, 1013,  410,  865,  865,   62,   62,   62,   62,   62,   62,   62,\n",
       "           62,   62,   62,   62,   62,   62],\n",
       "        [ 913,  424,  424,  424,  544,  544,  913,  765,  613,   95,  591,  697,\n",
       "          697,  697,  679,  578,  226,    8,  519,  648,  745,  334,  101,  399,\n",
       "           95,  703,  303,  419,  182,  466,  348,  794,  348,  648,  518,  544,\n",
       "          859,  511,  417,  633,  174,  869,  869,  512,  281,  669,  145, 1023,\n",
       "          228,  870,  685,  558,  762,  306,  475,   27,  619,  280,  694,  794,\n",
       "          149,  567,  826,  729,  811,  969,  969,  913,  518,  363,  841,  424,\n",
       "          518,  518,  518,  913,  913,  913,  424,  424,  424,  424,  424,  424,\n",
       "          424,  518,  424,  424,  424,  424,  424,  424,  424,  424,  424,  424,\n",
       "          424,  424,  424,  424,  424,  424,  424,  424,  424,  424,  424,  424,\n",
       "          941,   92,  481,  200,  228,  979,  956,   98,  271,  869,  869,  494,\n",
       "          567, 1008,  662,  662,  399,  399,  322,   21,  399,  399,  399,  975,\n",
       "          399,  828,  214,  703,    8,  979,  182,  269,  654,  558,  486,  569,\n",
       "          499,  489,  570,  806,  104,   79,  795,  420,  766,  822,  822,  684,\n",
       "          868,  344,  438,  438,  770,  564,   37,  815,  559,  990,  828,  512,\n",
       "          512,  512,  434,  434,  694,    8,  581,  973, 1023,  831, 1010,  646,\n",
       "          747,  747,  921,  993,  424,  937,  841,  214,  527,  511,  956,  460,\n",
       "           36,  460,  877,  651,  452,  831,  831,  646,  241,  386,  685,  413,\n",
       "          322,  575,  248,  575,  822,  906,  822,  812,  290,  822,  214,  822,\n",
       "          467,  373,  619,  229,  961, 1002,  544,  841,  424,  913,  518,  518,\n",
       "          518,  518,  424,  937,  302,  859,  913,  822,  645,  473,  347,  858,\n",
       "          278,  738,  470, 1010, 1010,  646,  831,  646,  993,  993,  859,  125,\n",
       "          213,  278,  348,  475,  697,  697,  420,  822,  822,  235,  822,  121,\n",
       "          769,  387,    3,  866,  128,  412,  227, 1010,  831, 1023,  993,  348,\n",
       "          646,  638,  875,  630,  966,  394,  869,  869,  136,  486,  486,  486,\n",
       "          486,  486,  132,  269,  440,  118,  736,  795,  214,  315,  931,  751,\n",
       "          349,  288,  742,  227,  913,  747,  691,  645,   94,  993,  913,  713,\n",
       "          775,  697,  322,  751,  300,  734,  736,  564,  728,  308,  869,   36,\n",
       "          346,   69,  956,  893,  672,  697,  443,  195,  736,  499,  682,  734,\n",
       "          734,  233,  871,  233,  420,  800,  893,  697,   89,  697,  630,  630,\n",
       "          986,   42,  643,  381,  151,  869,  833,  408,  544,  518, 1002,  729,\n",
       "          729, 1023,  833,  729,  833,  200,  841,  729,   43,   43,  729,  859,\n",
       "          740,  937,  765,  669,  821,   56,  396,  271,   55,  248,  118,  438,\n",
       "          822,  431,  869,   73,  555,  775,  343,  797,  306,  882,  466,  738,\n",
       "          602,  478,  399,  478,  529, 1002,  456,  456,  959,  820,  762,  392,\n",
       "          307, 1009,  438,  156,  322,  322,   98,  102,  703,  773,  537,  159,\n",
       "          979,  757,  286,  855,  671,  671,  322,  662,  662,   95,  225,  651,\n",
       "          762,  211,  322,  244,  662,  662,  478,  322,  662,  921,  723,  756,\n",
       "          981,  806,  544,  544,  518,  518,  518,  518,  518,  518,  518,  518,\n",
       "          518,  518,  518,  518,  518,  518]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7727,  8.4915, -3.5145,  ..., -0.6387, -1.6899,  1.7921],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         ...,\n",
       "         [-0.5899,  8.6741, -3.5319,  ..., -0.5725, -1.6565,  1.6399],\n",
       "         [-0.5192,  8.7648, -3.5058,  ..., -0.5464, -1.6215,  1.6735],\n",
       "         [-0.7237,  8.6417, -3.5161,  ..., -0.6146, -1.5997,  1.6597]],\n",
       "\n",
       "        [[-0.7727,  8.4915, -3.5145,  ..., -0.6387, -1.6899,  1.7921],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         [-0.5681,  8.6146, -3.5041,  ..., -0.5705, -1.7116,  1.8059],\n",
       "         ...,\n",
       "         [-0.6388,  8.5239, -3.5303,  ..., -0.5966, -1.7466,  1.7723],\n",
       "         [-0.6388,  8.5239, -3.5303,  ..., -0.5966, -1.7466,  1.7723],\n",
       "         [-0.6388,  8.5239, -3.5303,  ..., -0.5966, -1.7466,  1.7723]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embed*codes_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 147620\n"
     ]
    }
   ],
   "source": [
    "_, channels, input_length = inputs.input_values.shape\n",
    "\n",
    "chunk_length = input_length\n",
    "stride = input_length\n",
    "\n",
    "step = chunk_length - stride\n",
    "\n",
    "for offset in range(0, input_length - step, stride):\n",
    "    print(offset, offset + chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['audio_codes', 'audio_scales'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = model.decode(res.audio_codes, res.audio_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "for frame, scale in zip(res.audio_codes, res.audio_scales):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.chunk_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 462])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 147620])\n",
      "torch.Size([3, 128, 462])\n",
      "torch.Size([2, 3, 462])\n",
      "torch.Size([3, 128, 462])\n"
     ]
    }
   ],
   "source": [
    "bandwidth = model.config.target_bandwidths[0]\n",
    "embeddings = model.encoder(inputs.input_values)\n",
    "codes = model.quantizer.encode(embeddings, bandwidth)\n",
    "embedded = model.quantizer.decode(codes)\n",
    "print(inputs.input_values.shape)\n",
    "print(embeddings.shape)\n",
    "print(codes.shape)\n",
    "print(embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4, 9, 8, 7, 0, 0, 0],\n",
       "         [1, 2, 4, 3, 9, 8, 7, 9, 1, 0],\n",
       "         [1, 2, 3, 4, 9, 8, 7, 9, 1, 8]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([[[1, 2, 3, 4, 9, 8, 7, 4, 4, 4],\n",
    "                   [1, 2, 4, 3, 9, 8, 7, 9, 1, 4],\n",
    "                   [1, 2, 3, 4, 9, 8, 7, 9, 1, 8]]])\n",
    "torch.tensor([[[1,1,1,1,1,1,1,0,0,0], [1,1,1,1,1,1,1,1,1,0], [1,1,1,1,1,1,1,1,1,1]]]).bool()*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the tensor\n",
    "x = torch.tensor([[1, 2, 3, 4, 9, 8, 7, 4, 4, 4],\n",
    "                   [1, 2, 4, 3, 9, 8, 7, 9, 1, 4],\n",
    "                   [1, 2, 3, 4, 9, 8, 7, 9, 1, 8]])\n",
    "\n",
    "# Check if elements are not equal to 4\n",
    "not_fours = x != 4\n",
    "\n",
    "# Reverse the tensor along the last dimension to start checking from the end\n",
    "reversed_not_fours = not_fours.flip(dims=[-1])\n",
    "\n",
    "# Use cummax to propagate the first True value encountered (from the end) across all subsequent positions\n",
    "cumulative_mask = reversed_not_fours.cummax(dim=-1)[0]\n",
    "\n",
    "# Reverse the mask back to the original order\n",
    "final_mask = cumulative_mask.flip(dims=[-1])\n",
    "\n",
    "print(final_mask.int())  # Display the mask as integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1146, 0.9024, 0.9430, 0.0764],\n",
       "         [0.2286, 0.8260, 0.3325, 0.6528],\n",
       "         [0.0643, 0.7493, 0.2218, 0.1119],\n",
       "         [0.7053, 0.6366, 0.7577, 0.3341],\n",
       "         [0.3278, 0.3578, 0.8157, 0.2422],\n",
       "         [0.9998, 0.4148, 0.9526, 0.1089],\n",
       "         [0.6703, 0.8469, 0.9908, 0.6843],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5292, 0.4454, 0.1807, 0.9610],\n",
       "         [0.9582, 0.2388, 0.9461, 0.7105],\n",
       "         [0.2205, 0.7982, 0.9818, 0.5728],\n",
       "         [0.5858, 0.1187, 0.9630, 0.0422],\n",
       "         [0.3760, 0.2717, 0.7077, 0.6838],\n",
       "         [0.0281, 0.6394, 0.7935, 0.4989],\n",
       "         [0.2881, 0.1807, 0.6601, 0.7565],\n",
       "         [0.9185, 0.7074, 0.0926, 0.5016],\n",
       "         [0.0364, 0.9290, 0.1171, 0.6538],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1085, 0.4844, 0.2439, 0.3319],\n",
       "         [0.7462, 0.7410, 0.3246, 0.8543],\n",
       "         [0.1195, 0.6990, 0.3541, 0.9120],\n",
       "         [0.7502, 0.7814, 0.4015, 0.2222],\n",
       "         [0.6050, 0.3454, 0.3232, 0.3923],\n",
       "         [0.2504, 0.3731, 0.0419, 0.4843],\n",
       "         [0.9474, 0.7863, 0.8625, 0.5403],\n",
       "         [0.1830, 0.0197, 0.5987, 0.6639],\n",
       "         [0.0726, 0.7396, 0.4937, 0.0499],\n",
       "         [0.5023, 0.7285, 0.1576, 0.9457]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mask.unsqueeze(-1).expand(-1, -1, 4).float() * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1146, 0.9024, 0.9430, 0.0764],\n",
       "         [0.2286, 0.8260, 0.3325, 0.6528],\n",
       "         [0.0643, 0.7493, 0.2218, 0.1119],\n",
       "         [0.7053, 0.6366, 0.7577, 0.3341],\n",
       "         [0.3278, 0.3578, 0.8157, 0.2422],\n",
       "         [0.9998, 0.4148, 0.9526, 0.1089],\n",
       "         [0.6703, 0.8469, 0.9908, 0.6843],\n",
       "         [0.5544, 0.1763, 0.0176, 0.3314],\n",
       "         [0.7807, 0.3720, 0.4103, 0.5085],\n",
       "         [0.4169, 0.5296, 0.3217, 0.6099]],\n",
       "\n",
       "        [[0.5292, 0.4454, 0.1807, 0.9610],\n",
       "         [0.9582, 0.2388, 0.9461, 0.7105],\n",
       "         [0.2205, 0.7982, 0.9818, 0.5728],\n",
       "         [0.5858, 0.1187, 0.9630, 0.0422],\n",
       "         [0.3760, 0.2717, 0.7077, 0.6838],\n",
       "         [0.0281, 0.6394, 0.7935, 0.4989],\n",
       "         [0.2881, 0.1807, 0.6601, 0.7565],\n",
       "         [0.9185, 0.7074, 0.0926, 0.5016],\n",
       "         [0.0364, 0.9290, 0.1171, 0.6538],\n",
       "         [0.5288, 0.6977, 0.7585, 0.5893]],\n",
       "\n",
       "        [[0.1085, 0.4844, 0.2439, 0.3319],\n",
       "         [0.7462, 0.7410, 0.3246, 0.8543],\n",
       "         [0.1195, 0.6990, 0.3541, 0.9120],\n",
       "         [0.7502, 0.7814, 0.4015, 0.2222],\n",
       "         [0.6050, 0.3454, 0.3232, 0.3923],\n",
       "         [0.2504, 0.3731, 0.0419, 0.4843],\n",
       "         [0.9474, 0.7863, 0.8625, 0.5403],\n",
       "         [0.1830, 0.0197, 0.5987, 0.6639],\n",
       "         [0.0726, 0.7396, 0.4937, 0.0499],\n",
       "         [0.5023, 0.7285, 0.1576, 0.9457]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((3, 10, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncodecConfig {\n",
       "  \"_name_or_path\": \"facebook/encodec_24khz\",\n",
       "  \"architectures\": [\n",
       "    \"EncodecModel\"\n",
       "  ],\n",
       "  \"audio_channels\": 1,\n",
       "  \"chunk_length_s\": null,\n",
       "  \"codebook_dim\": 128,\n",
       "  \"codebook_size\": 1024,\n",
       "  \"compress\": 2,\n",
       "  \"dilation_growth_rate\": 2,\n",
       "  \"hidden_size\": 128,\n",
       "  \"kernel_size\": 7,\n",
       "  \"last_kernel_size\": 7,\n",
       "  \"model_type\": \"encodec\",\n",
       "  \"norm_type\": \"weight_norm\",\n",
       "  \"normalize\": false,\n",
       "  \"num_filters\": 32,\n",
       "  \"num_lstm_layers\": 2,\n",
       "  \"num_residual_layers\": 1,\n",
       "  \"overlap\": null,\n",
       "  \"pad_mode\": \"reflect\",\n",
       "  \"residual_kernel_size\": 3,\n",
       "  \"sampling_rate\": 24000,\n",
       "  \"target_bandwidths\": [\n",
       "    1.5,\n",
       "    3.0,\n",
       "    6.0,\n",
       "    12.0,\n",
       "    24.0\n",
       "  ],\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"trim_right_ratio\": 1.0,\n",
       "  \"upsampling_ratios\": [\n",
       "    8,\n",
       "    5,\n",
       "    4,\n",
       "    2\n",
       "  ],\n",
       "  \"use_causal_conv\": true,\n",
       "  \"use_conv_shortcut\": true\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True,  True,  True, False,  True,  True,  True],\n",
       "         [False,  True,  True,  True,  True,  True,  True, False,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_not_fours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.configs import SERConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SERConfig(num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([1,1,1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhnguyenhoang/Desktop/vscode/speech-language-foundation/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sampling_rate = torchaudio.load(\"e_happy.wav\")\n",
    "# print(sampling_rate)\n",
    "# print(waveform.shape)\n",
    "waveform = torchaudio.functional.resample(waveform, orig_freq=sampling_rate, new_freq=16000)\n",
    "# print(waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([111828])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor([waveform[0].numpy(), waveform[0][:-30].numpy()], sampling_rate=16000, padding=True, return_tensors=\"pt\", return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-0.0043, -0.0016, -0.0006,  ..., -0.0319,  0.1366,  0.0758],\n",
       "        [-0.0044, -0.0017, -0.0006,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 111828])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
